{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX+8UAnkynb48kZMJkrOjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhyeyppatel/Natural-Language-Processing/blob/main/Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9c5tj4zkBWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fb038b0",
        "outputId": "44b479ec-a3ec-4705-9bce-0d48e154b74a"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing numbers and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Removing extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Spelling correction (can be slow for large texts)\n",
        "    # text = str(TextBlob(text).correct())\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = text.split() # Simple split for demonstration\n",
        "\n",
        "    # Removing stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Lemmatization using spaCy\n",
        "    # Note: Stemming and Lemmatization are often used alternatively,\n",
        "    # spaCy's lemmatization is generally preferred for better accuracy\n",
        "    # doc = nlp(\" \".join(tokens))\n",
        "    # tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"This is an example sentence with some Punctuation! and numbers like 123. It also has some words that need correction, like 'wrds' and 'procesing'.\"\n",
        "processed_text = preprocess_text(sample_text)\n",
        "print(\"Original Text:\", sample_text)\n",
        "print(\"Processed Text:\", processed_text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: This is an example sentence with some Punctuation! and numbers like 123. It also has some words that need correction, like 'wrds' and 'procesing'.\n",
            "Processed Text: exampl sentenc punctuat number like also word need correct like wrd proces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install -q nltk textblob spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "\n",
        "# Step 3: Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Step 4: Preprocessing function\n",
        "def preprocess_text(text, do_spelling_correction=True):\n",
        "    print(\"Original Text:\", text)\n",
        "\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove numbers and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Spelling correction (optional)\n",
        "    if do_spelling_correction:\n",
        "        text = str(TextBlob(text).correct())\n",
        "\n",
        "    # Tokenization (simple split)\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Lemmatization using spaCy\n",
        "    doc = nlp(\" \".join(stemmed_tokens))\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "    # Final cleaned text\n",
        "    final_text = \" \".join(lemmatized_tokens)\n",
        "    print(\"Processed Text:\", final_text)\n",
        "    return final_text\n",
        "\n",
        "# Step 5: Run on sample text\n",
        "sample_text = \"This is an example sentence with some Punctuation! and numbers like 123. It also has some words that need correction, like 'wrds' and 'procesing'.\"\n",
        "preprocess_text(sample_text, do_spelling_correction=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "BDmsh9Rekv-k",
        "outputId": "3a405a41-49e6-4a30-e39b-cf633b84114d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: This is an example sentence with some Punctuation! and numbers like 123. It also has some words that need correction, like 'wrds' and 'procesing'.\n",
            "Processed Text: exampl sentenc punctuat number like also word need correct like word process\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'exampl sentenc punctuat number like also word need correct like word process'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63dd3799"
      },
      "source": [
        "Let's start by installing the necessary libraries: NLTK for tokenization, stop words, stemming, and lemmatization, and spaCy for a more robust approach to lemmatization and other tasks. We'll also install `textblob` for spelling correction."
      ]
    }
  ]
}